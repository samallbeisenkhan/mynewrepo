{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "–ï—Å–µ–ø 1. –ê–Ω—ã“õ—Ç–∞–º–∞–ª–∞—Ä\n",
        "\n",
        "–ê–Ω—Å–∞–º–±–ª—å ”ô–¥—ñ—Å—ñ: –±—ñ—Ä–Ω–µ—à–µ –º–æ–¥–µ–ª—å–¥—ñ“£ –±–æ–ª–∂–∞–º–¥–∞—Ä—ã–Ω –±—ñ—Ä—ñ–∫—Ç—ñ—Ä—ñ–ø (voting/averaging/weighting/stacking), –±—ñ—Ä –æ—Ä—Ç–∞“õ —à–µ—à—ñ–º —à—ã“ì–∞—Ä–∞—Ç—ã–Ω —Ç”ô—Å—ñ–ª.\n",
        "\n",
        "–ù–µ–≥–µ –±—ñ—Ä–Ω–µ—à–µ –º–æ–¥–µ–ª—å –±—ñ—Ä –º–æ–¥–µ–ª—å–¥–µ–Ω –∂–∞“õ—Å—ã –∂“±–º—ã—Å —ñ—Å—Ç–µ—É—ñ –º“Ø–º–∫—ñ–Ω: “õ–∞—Ç–µ–ª–µ—Ä—ñ–Ω ¬´–æ—Ä—Ç–∞—à–∞¬ª “õ—ã–ª–∞–¥—ã; –º–æ–¥–µ–ª—å–¥–µ—Ä ”ô—Ä—Ç“Ø—Ä–ª—ñ “õ–∞—Ç–µ –∂—ñ–±–µ—Ä—Å–µ, –±—ñ—Ä—ñ–∫—Ç—ñ—Ä—É variance-—Ç—ñ –∞–∑–∞–π—Ç—ã–ø, –∂–∞–ª–ø—ã–ª–∞—É–¥—ã –∫“Ø—à–µ–π—Ç–µ–¥—ñ.\n",
        "\n",
        "Bias / Variance:\n",
        "\n",
        "Bias: –º–æ–¥–µ–ª—å–¥—ñ“£ —à–∞–º–∞–¥–∞–Ω —Ç—ã—Å “õ–∞—Ä–∞–ø–∞–π—ã–º–¥—ã–ª—ã“ì—ã–Ω–∞–Ω —Ç—É–∞—Ç—ã–Ω –∂“Ø–π–µ–ª—ñ “õ–∞—Ç–µ (underfitting).\n",
        "\n",
        "Variance: –¥–µ—Ä–µ–∫–∫–µ —Ç—ã–º “õ–∞—Ç—Ç—ã —Ç”ô—É–µ–ª–¥—ñ–ª—ñ–∫, –æ“õ—ã—Ç—É –∂–∏—ã–Ω—Ç—ã“ì—ã–Ω–¥–∞“ì—ã ”©–∑–≥–µ—Ä—ñ—Å–∫–µ —Å–µ–∑—ñ–º—Ç–∞–ª–¥—ã“õ (overfitting).\n",
        "\n",
        "–ï—Å–µ–ø 2. Bagging –∏–¥–µ—è—Å—ã\n",
        "\n",
        "Bootstrap sample: –±–∞—Å—Ç–∞–ø“õ—ã train –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ–Ω–µ–Ω “õ–∞–π—Ç–∞ —Ç–∞“£–¥–∞—É–º–µ–Ω (replacement) –∞–ª—ã–Ω“ì–∞–Ω —Å–æ–ª –∫”©–ª–µ–º–¥–µ–≥—ñ “Ø–ª–≥—ñ.\n",
        "\n",
        "Bagging “õ–∞–Ω–¥–∞–π –ø—Ä–æ–±–ª–µ–º–∞–Ω—ã –∞–∑–∞–π—Ç–∞–¥—ã: –Ω–µ–≥—ñ–∑—ñ–Ω–µ–Ω variance (overfitting —Ç”ô—É–µ–∫–µ–ª—ñ).\n",
        "\n",
        "Bagging –ø–∞—Ä–∞–ª–ª–µ–ª—å –æ—Ä—ã–Ω–¥–∞–ª–∞ –º–∞? –ù–µ–≥–µ?: –∏”ô, ”ô—Ä –º–æ–¥–µ–ª—å —Ç”ô—É–µ–ª—Å—ñ–∑ bootstrap-—Ç–∞ –æ“õ—ã—Ç—ã–ª–∞–¥—ã, –±—ñ—Ä-–±—ñ—Ä—ñ–Ω–µ —Ç”ô—É–µ–ª–¥—ñ –µ–º–µ—Å.\n",
        "\n",
        "–ï—Å–µ–ø 3. Random Forest –µ—Ä–µ–∫—à–µ–ª—ñ–≥—ñ\n",
        "\n",
        "Bootstrap sampling ‚Äî —Ç”ô–Ω\n",
        "\n",
        "Feature randomness ‚Äî —Ç”ô–Ω\n",
        "\n",
        "Sequential learning ‚Äî —Ç”ô–Ω –µ–º–µ—Å\n",
        "\n",
        "Decision trees ‚Äî —Ç”ô–Ω\n",
        "\n",
        "Gradient optimization ‚Äî —Ç”ô–Ω –µ–º–µ—Å\n",
        "\n",
        "üü° 2-–¥–µ“£–≥–µ–π. –ï—Å–µ–ø—Ç–µ—É –∂”ô–Ω–µ –ª–æ–≥–∏–∫–∞\n",
        "–ï—Å–µ–ø 4. Majority voting\n",
        "\n",
        "–ë–µ—Ä—ñ–ª–≥–µ–Ω: M1=1, M2=0, M3=1\n",
        "\n",
        "–ê–Ω—Å–∞–º–±–ª—å –±–æ–ª–∂–∞–º—ã: 1 (–∫”©–ø—à—ñ–ª—ñ–∫ 1).\n",
        "\n",
        "–ë—ñ—Ä –º–æ–¥–µ–ª—å —Å–∞–ª–º–∞“ì—ã 2 –µ—Å–µ “Ø–ª–∫–µ–Ω –±–æ–ª—Å–∞ ”©–∑–≥–µ—Ä–µ –º–µ?\n",
        "\n",
        "–ï–≥–µ—Ä —Å–∞–ª–º–∞“ì—ã 2 –µ—Å–µ –º–æ–¥–µ–ª—å M2 –±–æ–ª—Å–∞: —Å–∞–ª–º–∞“õ—Ç–∞—Ä (M1=1, M2=2, M3=1) ‚Üí 0 –¥–∞—É—ã—Å—ã=2, 1 –¥–∞—É—ã—Å—ã=2 ‚Üí —Ç–µ“£ —Ç“Ø—Å–µ–¥—ñ (tie). Tie-breaking –µ—Ä–µ–∂–µ—Å—ñ–Ω–µ –±–∞–π–ª–∞–Ω—ã—Å—Ç—ã (”ô–¥–µ—Ç—Ç–µ –∞–ª–¥—ã–Ω –∞–ª–∞ –∫–µ–ª—ñ—Å—ñ–º –∫–µ—Ä–µ–∫).\n",
        "\n",
        "–ï–≥–µ—Ä —Å–∞–ª–º–∞“ì—ã 2 –µ—Å–µ –º–æ–¥–µ–ª—å M1 –Ω–µ–º–µ—Å–µ M3 –±–æ–ª—Å–∞: 1 –¥–∞—É—ã—Å—ã –±–∞—Å—ã–º –±–æ–ª—ã–ø “õ–∞–ª–∞–¥—ã ‚Üí –Ω”ô—Ç–∏–∂–µ 1."
      ],
      "metadata": {
        "id": "GMsJko4h9-In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ï—Å–µ–ø 6. Bagging (Python) ‚Äî Decision Tree base, n_estimators=30, accuracy, –±—ñ—Ä –∞“ì–∞—à–ø–µ–Ω —Å–∞–ª—ã—Å—Ç—ã—Ä—É"
      ],
      "metadata": {
        "id": "ZhjS-KxH-MGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# 1) Dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 2) Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, pred_dt)\n",
        "\n",
        "# 3) Bagging with Decision Tree base model\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=30,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "pred_bag = bag.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, pred_bag)\n",
        "\n",
        "print(f\"Single Decision Tree accuracy: {acc_dt:.4f}\")\n",
        "print(f\"Bagging (30 trees) accuracy:   {acc_bag:.4f}\")\n",
        "print(f\"Accuracy difference (Bag - DT): {acc_bag - acc_dt:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-4qOrAB-LP-",
        "outputId": "3ebf9b0e-3c9d-4390-f56c-935dd5d10374"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree accuracy: 0.9123\n",
            "Bagging (30 trees) accuracy:   0.9386\n",
            "Accuracy difference (Bag - DT): +0.0263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ï—Å–µ–ø 7. Random Forest –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä—ñ\n",
        "\n",
        "n_estimators –∞—Ä—Ç—Å–∞ –Ω–µ –±–æ–ª–∞–¥—ã?: ”ô–¥–µ—Ç—Ç–µ variance –∞–∑–∞—è–¥—ã, accuracy —Ç“±—Ä–∞“õ—Ç–∞–Ω–∞–¥—ã; —É–∞“õ—ã—Ç/–∂–∞–¥—ã —à—ã“ì—ã–Ω—ã ”©—Å–µ–¥—ñ; –±–µ–ª–≥—ñ–ª—ñ –±—ñ—Ä –º–µ–∂–µ–¥–µ–Ω –∫–µ–π—ñ–Ω “±—Ç—ã—Å –∞–∑–∞—è–¥—ã.\n",
        "\n",
        "max_depth –∞–∑–∞–π—Å–∞ ”ô—Å–µ—Ä—ñ: –∞“ì–∞—à—Ç–∞—Ä “õ–∞—Ä–∞–ø–∞–π—ã–º–¥–∞–π–¥—ã ‚Üí bias ”©—Å–µ–¥—ñ, variance –∞–∑–∞—è–¥—ã; overfitting —Ç”©–º–µ–Ω–¥–µ–π–¥—ñ, –±—ñ—Ä–∞“õ underfitting —Ç—É—ã–Ω–¥–∞—É—ã –º“Ø–º–∫—ñ–Ω.\n",
        "\n",
        "–ù–µ–ª—ñ–∫—Ç–µ–Ω Random Forest overfitting-–∫–µ —Ç”©–∑—ñ–º–¥—ñ?: bootstrap + feature randomness –∞“ì–∞—à—Ç–∞—Ä–¥—ã ”ô—Ä—Ç“Ø—Ä–ª—ñ “õ—ã–ª–∞–¥—ã; –æ—Ä—Ç–∞—à–∞/–¥–∞—É—ã—Å –±–µ—Ä—É variance-—Ç—ñ —Ç“Ø—Å—ñ—Ä–µ–¥—ñ."
      ],
      "metadata": {
        "id": "r3-W6I12-fuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ 4-–¥–µ“£–≥–µ–π. Gradient Boosting\n",
        "–ï—Å–µ–ø 8. –ë—É—Å—Ç–∏–Ω–≥ –ª–æ–≥–∏–∫–∞—Å—ã\n",
        "\n",
        "–ù–µ–ª—ñ–∫—Ç–µ–Ω Gradient Boosting sequential?: ”ô—Ä –∂–∞“£–∞ –º–æ–¥–µ–ª—å –∞–ª–¥—ã“£“ì—ã–ª–∞—Ä–¥—ã“£ “õ–∞—Ç–µ–ª–µ—Ä—ñ–Ω —Ç“Ø–∑–µ—Ç—É “Ø—à—ñ–Ω (residual/–≥—Ä–∞–¥–∏–µ–Ω—Ç –±–∞“ì—ã—Ç—ã–º–µ–Ω) “Ø–π—Ä–µ–Ω–µ–¥—ñ, —Å–æ–Ω–¥—ã“õ—Ç–∞–Ω —Ç”ô—É–µ–ª–¥—ñ “õ–∞–¥–∞–º–¥–∞—Ä —Ç—ñ–∑–±–µ–≥—ñ –ø–∞–π–¥–∞ –±–æ–ª–∞–¥—ã.\n",
        "\n",
        "Residuals (“õ–∞–ª–¥—ã“õ—Ç–∞—Ä): –∞–ª–¥—ã“£“ì—ã –º–æ–¥–µ–ª—å–¥—ñ“£ “õ–∞—Ç–µ–ª—ñ–≥—ñ; –±–∏–Ω–∞—Ä/–∫”©–ø–∫–ª–∞—Å—Å—Ç–∞ ‚Äî loss —Ñ—É–Ω–∫—Ü–∏—è—Å—ã–Ω—ã“£ —Ç–µ—Ä—ñ—Å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ñ —Ç“Ø—Ä—ñ–Ω–¥–µ “õ–æ–ª–¥–∞–Ω—ã–ª–∞–¥—ã.\n",
        "\n",
        "Learning rate —Ä”©–ª—ñ: ”ô—Ä “õ–∞–¥–∞–º–Ω—ã“£ “Ø–ª–µ—Å—ñ–Ω ¬´–±”ô—Å–µ“£–¥–µ—Ç–µ–¥—ñ¬ª; –∫—ñ—à—ñ—Ä–µ–∫ rate ‚Üí —Ç“±—Ä–∞“õ—Ç—ã—Ä–∞“õ, –±—ñ—Ä–∞“õ –∫”©–±—ñ—Ä–µ–∫ estimator –∫–µ—Ä–µ–∫; —Ç—ã–º “Ø–ª–∫–µ–Ω rate ‚Üí overfitting/—Ç“±—Ä–∞“õ—Å—ã–∑–¥—ã“õ “õ–∞—É–ø—ñ."
      ],
      "metadata": {
        "id": "VUFu0uK0-kse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ï—Å–µ–ø 9. Gradient Boosting (Python) ‚Äî n_estimators=50,100,200 —Å–∞–ª—ã—Å—Ç—ã—Ä—É"
      ],
      "metadata": {
        "id": "r3usQxWPADEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_list = [50, 100, 200]\n",
        "results = {}\n",
        "\n",
        "for n in n_list:\n",
        "    gb = GradientBoostingClassifier(n_estimators=n, random_state=42)\n",
        "    gb.fit(X_train, y_train)\n",
        "    pred = gb.predict(X_test)\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    results[n] = acc\n",
        "    print(f\"GradientBoosting n_estimators={n:3d} -> accuracy={acc:.4f}\")\n",
        "\n",
        "best_n = max(results, key=results.get)\n",
        "print(f\"\\nBest n_estimators by test accuracy: {best_n} (acc={results[best_n]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KchSMM5ACdv",
        "outputId": "abf6d078-6c52-48a0-917d-ce9270aeff74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoosting n_estimators= 50 -> accuracy=0.9474\n",
            "GradientBoosting n_estimators=100 -> accuracy=0.9561\n",
            "GradientBoosting n_estimators=200 -> accuracy=0.9561\n",
            "\n",
            "Best n_estimators by test accuracy: 100 (acc=0.9561)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üî¥ 5-–¥–µ“£–≥–µ–π. Stacking\n",
        "–ï—Å–µ–ø 10. Stacking “õ“±—Ä—ã–ª—ã–º—ã\n",
        "\n",
        "Level-0 –º–æ–¥–µ–ª—å–¥–µ—Ä: Decision Tree, Random Forest, SVM (–±“±–ª–∞—Ä –±–∞–∑–∞–ª—ã“õ –º–æ–¥–µ–ª—å–¥–µ—Ä).\n",
        "\n",
        "–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: Logistic Regression (–∂–∏—ñ), –Ω–µ–º–µ—Å–µ Linear model, Gradient Boosting, —Ç.–±.\n",
        "\n",
        "–ù–µ–ª—ñ–∫—Ç–µ–Ω train –¥–µ—Ä–µ–∫—Ç–µ—Ä–¥—ñ —Ç—ñ–∫–µ–ª–µ–π “õ–æ–ª–¥–∞–Ω—É“ì–∞ –±–æ–ª–º–∞–π–¥—ã?: meta-model –±–∞–∑–∞–ª—ã“õ –º–æ–¥–µ–ª—å–¥–µ—Ä–¥—ñ“£ train-–¥–µ–≥—ñ ¬´—Ç—ã–º –∂–∞“õ—Å—ã¬ª –±–æ–ª–∂–∞–º—ã–Ω–∞ “Ø–π—Ä–µ–Ω—ñ–ø, leakage –∞—Ä“õ—ã–ª—ã overfitting –∫“Ø—à–µ–π–µ–¥—ñ; –¥“±—Ä—ã—Å—ã ‚Äî out-of-fold –±–æ–ª–∂–∞–º."
      ],
      "metadata": {
        "id": "CoBWquTJZsxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ï—Å–µ–ø 11. Stacking (Python) ‚Äî ‚â•2 base model, Logistic Regression meta, accuracy, RF-–ø–µ–Ω —Å–∞–ª—ã—Å—Ç—ã—Ä—É"
      ],
      "metadata": {
        "id": "xgQ_z_ooZvGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 1) Dataset (moved from previous cell to ensure definition)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Base models\n",
        "estimators = [\n",
        "    (\"dt\", DecisionTreeClassifier(random_state=42)),\n",
        "    (\"svm\", SVC(probability=True, random_state=42))\n",
        "]\n",
        "\n",
        "# Meta model\n",
        "meta = LogisticRegression(max_iter=5000, random_state=42)\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "pred_stack = stack.predict(X_test)\n",
        "acc_stack = accuracy_score(y_test, pred_stack)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "pred_rf = rf.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, pred_rf)\n",
        "\n",
        "print(f\"Stacking accuracy:      {acc_stack:.4f}\")\n",
        "print(f\"RandomForest accuracy:  {acc_rf:.4f}\")\n",
        "print(f\"Difference (Stack - RF): {acc_stack - acc_rf:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tr3Xf0FZx_E",
        "outputId": "c1cd72b7-1e4d-4d83-aa99-7a1980c180eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking accuracy:      0.9211\n",
            "RandomForest accuracy:  0.9561\n",
            "Difference (Stack - RF): -0.0351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lQhE98hUaESM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü£ 6-–¥–µ“£–≥–µ–π. –ê–Ω–∞–ª–∏—Ç–∏–∫–∞–ª—ã“õ\n",
        "–ï—Å–µ–ø 12. Bias‚ÄìVariance —Å–∞–ª—ã—Å—Ç—ã—Ä—É\n",
        "\n",
        "Single Decision Tree: bias —Ç”©–º–µ–Ω/–æ—Ä—Ç–∞—à–∞, variance –∂–æ“ì–∞—Ä—ã.\n",
        "\n",
        "Bagging: bias —à–∞–º–∞–º–µ–Ω —Å–æ–ª –∫“Ø–π—ñ, variance —Ç”©–º–µ–Ω.\n",
        "\n",
        "Random Forest: bias —Å”ô–ª ”©—Å–µ–¥—ñ/“±“õ—Å–∞—Å, variance bagging-—Ç–µ–Ω –¥–µ —Ç”©–º–µ–Ω (feature randomness ”ô—Å–µ—Ä—ñ).\n",
        "\n",
        "Gradient Boosting: bias —Ç”©–º–µ–Ω–¥–µ–π–¥—ñ (–∫“Ø—à—Ç—ñ fit), variance ”©—Å—ñ–ø –∫–µ—Ç—É—ñ –º“Ø–º–∫—ñ–Ω (”ô—Å—ñ—Ä–µ—Å–µ “Ø–ª–∫–µ–Ω depth/–∫”©–ø estimator/“Ø–ª–∫–µ–Ω learning rate –∫–µ–∑—ñ–Ω–¥–µ).\n",
        "\n",
        "–ï—Å–µ–ø 13. “ö–∞–π ”ô–¥—ñ—Å—Ç—ñ —Ç–∞“£–¥–∞—É –∫–µ—Ä–µ–∫?\n",
        "\n",
        "–®–∞“ì—ã–Ω –¥–∞—Ç–∞—Å–µ—Ç: –∫”©–±—ñ–Ω–µ Random Forest (—Ç“±—Ä–∞“õ—Ç—ã), –Ω–µ–º–µ—Å–µ Bagging (“õ–∞—Ä–∞–ø–∞–π—ã–º).\n",
        "\n",
        "“Æ–ª–∫–µ–Ω noisy –¥–µ—Ä–µ–∫—Ç–µ—Ä: Random Forest –Ω–µ–º–µ—Å–µ Bagging (variance-—Ç—ñ –±–∞—Å–∞–¥—ã).\n",
        "\n",
        "Feature —Å–∞–Ω—ã ”©—Ç–µ –∫”©–ø: Random Forest (feature subsampling –ø–∞–π–¥–∞–ª—ã), –∫–µ–π–¥–µ linear + regularization (–µ–≥–µ—Ä ”©—Ç–µ sparse/–∂–æ“ì–∞—Ä—ã ”©–ª—à–µ–º).\n",
        "\n",
        "–ï“£ –∂–æ“ì–∞—Ä—ã accuracy “õ–∞–∂–µ—Ç: –∂–∏—ñ Gradient Boosting –Ω–µ–º–µ—Å–µ Stacking (–¥“±—Ä—ã—Å –±–∞–ø—Ç–∞—É–º–µ–Ω)."
      ],
      "metadata": {
        "id": "kGSY9VVxaDqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“ö–æ—Ä—ã—Ç—ã–Ω–¥—ã –µ—Å–µ–ø (Mini-project) ‚Äî –±—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—Ç–µ 4 ”ô–¥—ñ—Å —Å–∞–ª—ã—Å—Ç—ã—Ä—É"
      ],
      "metadata": {
        "id": "YJ6DvAUgaHcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def fit_eval(model, name):\n",
        "    t0 = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - t0\n",
        "    pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    return name, acc, train_time\n",
        "\n",
        "models = []\n",
        "\n",
        "# Bagging\n",
        "models.append((\n",
        "    \"Bagging\",\n",
        "    BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(random_state=42),\n",
        "        n_estimators=200,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "))\n",
        "\n",
        "# Random Forest\n",
        "models.append((\n",
        "    \"RandomForest\",\n",
        "    RandomForestClassifier(\n",
        "        n_estimators=500,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "))\n",
        "\n",
        "# Gradient Boosting\n",
        "models.append((\n",
        "    \"GradientBoosting\",\n",
        "    GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        random_state=42\n",
        "    )\n",
        "))\n",
        "\n",
        "# Stacking\n",
        "stack = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"rf\", RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)),\n",
        "        (\"svm\", SVC(probability=True, random_state=42))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=5000, random_state=42),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "models.append((\"Stacking\", stack))\n",
        "\n",
        "results = []\n",
        "for name, model in models:\n",
        "    results.append(fit_eval(model, name))\n",
        "\n",
        "# Display\n",
        "results_sorted_acc = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "results_sorted_time = sorted(results, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"=== Accuracy ranking (higher is better) ===\")\n",
        "for name, acc, t in results_sorted_acc:\n",
        "    print(f\"{name:16s}  acc={acc:.4f}  train_time={t:.3f}s\")\n",
        "\n",
        "print(\"\\n=== Training time ranking (slower first) ===\")\n",
        "for name, acc, t in results_sorted_time:\n",
        "    print(f\"{name:16s}  train_time={t:.3f}s  acc={acc:.4f}\")\n",
        "\n",
        "best = results_sorted_acc[0][0]\n",
        "slowest = results_sorted_time[0][0]\n",
        "print(f\"\\nBest accuracy: {best}\")\n",
        "print(f\"Slowest training: {slowest}\")\n",
        "\n",
        "# \"–ï“£ —Ç“±—Ä–∞“õ—Ç—ã\" (–∂–µ–¥–µ–ª —Ç–µ–∫—Å–µ—Ä—ñ—Å): ”ô—Ä –º–æ–¥–µ–ª—å–¥—ñ –±—ñ—Ä–Ω–µ—à–µ random_state-–ø–µ–Ω “õ–∞–π—Ç–∞–ª–∞–ø, accuracy std –µ—Å–µ–ø—Ç–µ—É\n",
        "def stability_test(make_model_fn, seeds=(0, 1, 2, 3, 4)):\n",
        "    accs = []\n",
        "    for s in seeds:\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=s, stratify=y)\n",
        "        m = make_model_fn(s)\n",
        "        m.fit(X_tr, y_tr)\n",
        "        accs.append(accuracy_score(y_te, m.predict(X_te)))\n",
        "    return float(np.mean(accs)), float(np.std(accs))\n",
        "\n",
        "stab = {}\n",
        "stab[\"Bagging\"] = stability_test(lambda s: BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=s),\n",
        "    n_estimators=200, random_state=s, n_jobs=-1\n",
        "))\n",
        "stab[\"RandomForest\"] = stability_test(lambda s: RandomForestClassifier(\n",
        "    n_estimators=500, random_state=s, n_jobs=-1\n",
        "))\n",
        "stab[\"GradientBoosting\"] = stability_test(lambda s: GradientBoostingClassifier(\n",
        "    n_estimators=200, random_state=s\n",
        "))\n",
        "stab[\"Stacking\"] = stability_test(lambda s: StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"rf\", RandomForestClassifier(n_estimators=200, random_state=s, n_jobs=-1)),\n",
        "        (\"svm\", SVC(probability=True, random_state=s))\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=5000, random_state=s),\n",
        "    cv=5, n_jobs=-1\n",
        "))\n",
        "\n",
        "print(\"\\n=== Stability (mean ¬± std over different splits) ===\")\n",
        "for k, (m, sd) in stab.items():\n",
        "    print(f\"{k:16s}  mean={m:.4f}  std={sd:.4f}\")\n",
        "\n",
        "most_stable = min(stab.items(), key=lambda kv: kv[1][1])[0]\n",
        "print(f\"\\nMost stable (lowest std): {most_stable}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqH3kRF0aIa8",
        "outputId": "a662cc3c-c88e-439c-a773-63fc5227d0f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Accuracy ranking (higher is better) ===\n",
            "RandomForest      acc=0.9561  train_time=1.624s\n",
            "GradientBoosting  acc=0.9561  train_time=2.027s\n",
            "Stacking          acc=0.9561  train_time=4.155s\n",
            "Bagging           acc=0.9474  train_time=1.320s\n",
            "\n",
            "=== Training time ranking (slower first) ===\n",
            "Stacking          train_time=4.155s  acc=0.9561\n",
            "GradientBoosting  train_time=2.027s  acc=0.9561\n",
            "RandomForest      train_time=1.624s  acc=0.9561\n",
            "Bagging           train_time=1.320s  acc=0.9474\n",
            "\n",
            "Best accuracy: RandomForest\n",
            "Slowest training: Stacking\n",
            "\n",
            "=== Stability (mean ¬± std over different splits) ===\n",
            "Bagging           mean=0.9667  std=0.0244\n",
            "RandomForest      mean=0.9702  std=0.0163\n",
            "GradientBoosting  mean=0.9649  std=0.0175\n",
            "Stacking          mean=0.9667  std=0.0129\n",
            "\n",
            "Most stable (lowest std): Stacking\n"
          ]
        }
      ]
    }
  ]
}